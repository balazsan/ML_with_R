{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<font size=6>Introduction to Neural Networks</font><br><br>\n",
    "\n",
    "* <font size=5>Artificial <b>Neural Networks</b> (NN) are computing systems vaguely inspired by the biological neural networks that form animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the results to identify cats in other images.</font><br><br>\n",
    "* <font size=5>A NN is based on a collection of connected units or nodes called neurons, which loosely model the neurons in a biological brain. These neurons are usually organized in layers.</font><br><br><a id='graph'></a>\n",
    "<img src=\"../aux_data/nn.png\" width=\"400\" align=\"right\"/>\n",
    "* <font size=5>A simple example of a NN is shown in the picture</font><br><br>\n",
    "    * <font size=5 color='red'>Input layer</font><font size=5>: the layer that receives external data, number of nodes (red circles) equals to the amount of sample plots imported into the network at the same time</font><br><br>\n",
    "    * <font size=5 color='blue'>Hidden layer</font><font size=5>: the layer between input and output, number of nodes (blue circles) can widely vary (one or many)</font><br><br>\n",
    "    * <font size=5 color='green'>Output layer</font><font size=5>: the layer that creates the results, number of nodes (green circles) corresponds to the amount of estimated attributes</font><br><br>\n",
    "\n",
    "* <font size=5>The layer is fully connected when every neuron in one layer is connected to every neuron in the next layer. The picture shows a fully connected hidden layer. Such a layer is going to be used in the exercise.</font><br><br>\n",
    "\n",
    "* <font size=5>Important parts of NNs are e.g. <b>neurons</b>, <b>connections and weights</b>, <b>hyperparameters</b>.</font>\n",
    "    * <font size=5>NNs are composed of <b>neurons</b>, which receive input, combine the input with their internal state (activation) and an optional threshold using an activation function, and produce output using an output function.</font><br><br>\n",
    "    * <font size=5>The network consists of <b>connections</b>, each connection providing the output of one neuron as an input to another neuron. Each connection is assigned a <b>weight</b> that represents its relative importance. A given neuron can have multiple input and output connections.</font><br><br>\n",
    "    * <font size=5>A <b>hyperparameter</b> is a constant parameter whose value is set before the <b>learning process</b> begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size.</font><br><br>\n",
    "\n",
    "* <font size=5><b>Learning</b> is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a <b>cost function</b> (also called <b>loss</b>) that is evaluated periodically during learning. As long as its output continues to decline, learning continues.</font><br><br>\n",
    "\n",
    "* <font size=5>The <b>learning rate</b> defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. It is also possible to use scheduled learning rate during the learning process. In this case the learning rate is decreasing as the learning progresses.</font><br><br>\n",
    "\n",
    "* <font size=5><b>Loss</b> is a quantity that needs to be optimized during the learning process (training). During training predicted values (output) and observed values are used to calculate the accuracy of the model. The calculation is done with the help of a loss funciton, which is selected according to the optimization problem. In our example (regression) Mean Squared Error (<b>MSE</b>) is generaly considered to be a good choice.<br><br>\n",
    "$$MSE = \\frac{1}{n} \\sum \\limits _{i=1} ^{p} (Y_{i}-\\hat{Y}_{i})^{2}$$\n",
    "<br>Where $Y$ is the vector of observed and $\\hat{Y}$ is the vector of predicted values. The length of the vectors (i.e. number of of observations/predictions) is $n$.<br>In our example the network is going to try to minimize the value of MSE.</font><img src=\"../aux_data/optimizer.gif\" align=\"right\"/><br><br>\n",
    "\n",
    "* <font size=5><b>Optimizers</b> help to update the weights of the network based on the loss function. There is a wide range of optimizers available. The animation below shows how different optimizers behave during training. The star in the center symbolizes the optimum solution of the problem (in this case where maximum accuracy can be reached). In the lower graph the x-axis is the number of training iterations (also called epochs) and the y-axis is the accuracy. In our example Adam optimizer is going to be used (not present in the animation).</font><br><br>\n",
    "<font size=3>(animation from https://imgur.com/s25RsOr)</font>\n",
    "\n",
    "<br><br><br><br><br><br><br><br>\n",
    "\n",
    "* <font size=5>Finally some thoughts about <b>over-</b> and <b>underfitting</b>.</font><br><br>\n",
    "    * <img src=\"../aux_data/overfit.png\" width=400 align=\"right\"/><font size=5><b>Overfitting</b> is the result of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably.</font><br><br><br><br><br><br><br><br>\n",
    "    * <img src=\"../aux_data/underfit.png\" width=400 align=\"right\"/><font size=5><b>Underfitting</b> occurs when a statistical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing.</font><br><br><br><br><br><br>\n",
    "    * <font size=5>In the exercise we are going to deal with overfitting. <b>Regularization</b> methods, such as use of validation data or early stopping help to prevent overfitting.</font><br><br>\n",
    "<font size=3>(images from <a href=\"https://towardsdatascience.com/what-are-overfitting-and-underfitting-in-machine-learning-a96b30864690\">towards data science article</a>)</font><br><br>\n",
    "\n",
    "<font size=3>(this notebook is based on: Wikipedia's article on <a href =\"https://en.wikipedia.org/wiki/Artificial_neural_network\">Artificial neural network</a>)</font>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
